AWSTemplateFormatVersion: "2010-09-09"
Description: "Create an S3 Bucket"


Resources:
    MyS3Bucket:
        Type: AWS::S3::Bucket
        DeletionPolicy: Delete
        Properties:
            BucketName: mergeddatar14
    
    GlueJobImport:
        Type: AWS::Glue::Job
        DependsOn: MyS3Bucket
        Properties:
            Name: data-importsep37
            Description: Ingests data from s3 and writes it as a parquet file to the data lake
            ExecutionClass: FLEX
            GlueVersion: 4.0
            MaxRetries: 0
            NumberOfWorkers: 6
            Role: arn:aws:iam::467280971091:role/LabRole
            Timeout: 40
            WorkerType: G.1X
            Command:
                Name: glueetl
                ScriptLocation: s3://github-scrpits/Datawarehouse_Script.py

    # LambdaTrigger:
    #     Type: AWS::Lambda::Function
    #     Properties:
    #         Code:
    #           ZipFile: |
    #               import boto3
    #               def lambda_handler(event, context):
    #                 glue_client = boto3.client('glue')
    #                 job_name = 'data-import4'
    #                 response = glue_client.start_job_run(JobName=job_name)
    #                 return {
    #                         'statusCode': 200,
    #                         'body': 'Glue Job started successfully.'
    #                     }
    #         FunctionName: Trigger52
    #         Handler: index.lambda_handler
    #         Role: arn:aws:iam::467280971091:role/LabRole
    #         Runtime: python3.10
    #         Timeout: 10
            

    GlueDatabase:
        Type: AWS::Glue::Database
        Properties:
            CatalogId: !Ref AWS::AccountId
            DatabaseInput:
                Name: database64  
    
    GlueCrawler:
        Type: AWS::Glue::Crawler
        Properties:
            Name: CrawlingData64
            DatabaseName:  database64
            Targets:
                S3Targets:
                    - Path: s3://mergeddatar14/datawarehouse/
            Role: arn:aws:iam::467280971091:role/LabRole
            
    
    WorkflowJob:
        Type: AWS::Glue::Workflow
        Properties:
            Description: Create workflow
            MaxConcurrentRuns: 1
            Name: Jobworkflow
            
    WorkflowStartTrigger:
        Type: AWS::Glue::Trigger
        Properties:
            Name: 'StartTrigger'
            Type: ON_DEMAND
            Description: 'Trigger to start the workflow'
            Actions:
              - JobName: !Ref data-importsep37
                WorkflowName: !Ref WorkflowJob
            
    CrawlerJobTrigger:
        Type: AWS::Glue::Trigger
        Properties:
            Name: jobsuccesfultrigger1
            Type: CONDITIONAL
            StartOnCreation: TRUE
            Description: Trigger to start the crawler
            Actions:
              - CrawlerName:  CrawlingData64
            Predicate:
              Conditions:
                - LogicalOperator: EQUALS
                  JobName: data-importsep37
                  State: SUCCEEDED
            WorkflowName: !Ref WorkflowJob

            
    MyAthenaWorkGroup:
        Type: AWS::Athena::WorkGroup
        Properties:
            Name: MyWorkGroup
            Description: workgroup for Athena
            State: ENABLED
            WorkGroupConfiguration:
                BytesScannedCutoffPerQuery: 200000000
                EnforceWorkGroupConfiguration: false
                PublishCloudWatchMetricsEnabled: true
                RequesterPaysEnabled: true
                ResultConfiguration:
                    OutputLocation: s3://finalbucketgrp4/    