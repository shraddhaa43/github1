AWSTemplateFormatVersion: "2010-09-09"
Description: "Create an S3 Bucket"
Resources:
    MyS3Bucket:
        Type: AWS::S3::Bucket
        DeletionPolicy: Retain
        Properties:
            BucketName: mergeddata10
    
    GlueJobImport:
        Type: AWS::Glue::Job
        DependsOn: MyS3Bucket
        Properties:
            Name: data-import2
            Description: Ingests data from s3 and writes it as a parquet file to the data lake
            ExecutionClass: FLEX
            GlueVersion: 4.0
            MaxRetries: 0
            NumberOfWorkers: 6
            Role: arn:aws:iam::467280971091:role/LabRole
            Timeout: 30
            WorkerType: G.1X
            Command:
                Name: glueetl
                ScriptLocation: s3://github-scrpits/Datawarehouse_Script.py

    LambdaTrigger:
        Type: AWS::Lambda::Function
        Properties:
            Code:
              ZipFile: |
                  import boto3
                  def lambda_handler(event, context):
                    glue_client = boto3.client('glue')
                    job_name = 'data-import2'
                    response = glue_client.start_job_run(JobName=job_name)
                    return {
                            'statusCode': 200,
                            'body': 'Glue Job started successfully.'
                        }
            FunctionName: Trigger1
            Handler: index.lambda_handler
            Role: arn:aws:iam::467280971091:role/LabRole
            Runtime: python3.10
            Timeout: 10

    GlueDatabase:
        Type: AWS::Glue::Database
        Properties:
            CatalogId: !Ref AWS::AccountId
            DatabaseInput:
                Name: database3  
    
    GlueCrawler:
        Type: AWS::Glue::Crawler
        Properties:
            Name: CrawlingData2
            DatabaseName: database3
            Targets:
                S3Targets:
                    - Path: s3://mergeddata10/datawarehouse/
            Role: arn:aws:iam::467280971091:role/LabRole